# Orchestrateur de Microservices Kafka

## ğŸ“– Vue dâ€™ensemble

Ce projet met en place un **orchestrateur** (un service de coordination) basÃ© sur **Apache Kafka ğŸ“¨**. Son rÃ´le est dâ€™organiser et de suivre le traitement dâ€™objets de donnÃ©es dÃ©crits dans le fichier **Objets.json ğŸ“‚**.

---

## ğŸ”§ En pratique

- âš™ï¸ Lâ€™orchestrateur dÃ©termine **dans quel ordre** les objets doivent Ãªtre traitÃ©s, en respectant leurs dÃ©pendances.
- ğŸ“¨ Il envoie des **messages dans Kafka**, ce qui permet aux diffÃ©rents traitements de sâ€™exÃ©cuter **en parallÃ¨le** et de maniÃ¨re **asynchrone**.
- ğŸ“Š Les objets passent par trois grandes Ã©tapes :
  1. **Ingestion ğŸ“¥**
  2. **Standardisation ğŸ”„**
  3. **Application ğŸš€**
- ğŸ’» Un **frontend** (interface web) permet de **suivre en direct** lâ€™avancement des traitements.
- ğŸ³ Le tout est dÃ©ployÃ© avec **Docker** et **Docker Compose**, ce qui facilite lâ€™installation et lâ€™exÃ©cution sur nâ€™importe quelle machine.

---

## ğŸ“Œ Points clÃ©s du projet

- ğŸ¯ Identifier les **objets finaux** (IDs â‰¥ 3000) et remonter leurs dÃ©pendances jusquâ€™aux **objets sources** (IDs 1000â€“1999).
- ğŸ§© Optimiser le **graphe des dÃ©pendances** pour Ã©viter de retraiter plusieurs fois les mÃªmes objets.
- ğŸ“¨ Publier des **Ã©vÃ©nements Kafka** (*ObjectReady*, *ObjectLoaded*, *ObjectFailed*) pour dÃ©clencher et suivre le traitement.
- ğŸ‘€ Offrir une bonne **visibilitÃ©** grÃ¢ce Ã  des **API ğŸŒ** et un **flux dâ€™Ã©vÃ©nements en temps rÃ©el ğŸ“¡**.

## ğŸ“Š Architecture du projet

```mermaid
flowchart LR
    subgraph Frontend
        UI["Interface Web (Suivi en temps rÃ©el)"]
    end

    subgraph Orchestrator
        O["Lecture Objets.json"]
        D["Gestion des dÃ©pendances (graphe optimisÃ©)"]
        E["Publication Ã©vÃ©nements Kafka (ObjectReady / Loaded / Failed)"]
    end

    subgraph Kafka
        K["Bus de messages"]
    end

    subgraph Producers
        P1["Ingestion"]
        P2["Standardisation"]
        P3["Application"]
    end

    O --> D --> E --> K
    K --> P1
    K --> P2
    K --> P3
    P1 --> K
    P2 --> K
    P3 --> K
    K --> UI

```

## ğŸ—‚ï¸ Structure du projet

Le projet est organisÃ© comme suit :

âš™ï¸ Orchestrateur (orchestrator/)

Microservice Python (FastAPI).

Charge le fichier Objets.json, construit un graphe de dÃ©pendances et publie les Ã©vÃ©nements ObjectReady dans Kafka en respectant lâ€™ordre topologique.

Contient le client Kafka, les modÃ¨les Pydantic et la logique dâ€™orchestration.

ğŸ“¨ Producteurs Kafka (producers/) (simulations en Python) :

producer_ingest.py : traite les objets 1000â€“1999 (â±ï¸ dÃ©lai simulÃ© : 0,5s).

producer_standardize.py : traite les objets 2000â€“2999 (â±ï¸ dÃ©lai simulÃ© : 0,6s, avec un Ã©chec forcÃ© pour lâ€™ID 2005).

producer_application.py : traite les objets 3000+ (â±ï¸ dÃ©lai simulÃ© : 0,8s).

ğŸ’» Frontend (frontend/)

Application Angular avec intÃ©gration SSE.

Utilise Nginx pour servir et proxy les Ã©vÃ©nements Kafka.

ğŸ—ï¸ Infrastructure

DÃ©ployÃ©e avec Docker Compose ğŸ³.

Inclut :

Zookeeper ğŸ¦“

Kafka ğŸ“¨

Lâ€™orchestrateur, les producteurs, et le frontend.

ğŸ“Š Prometheus
â€¢	RÃ´le : SystÃ¨me de monitoring et mÃ©triques.
â€¢	Scrape les mÃ©triques exposÃ©es par tes producteurs/orchestrateur (ex : via prometheus-client).
â€¢	Accessible sur port 9090 pour voir les mÃ©triques brutes. 

ğŸ“ˆ Grafana
â€¢	RÃ´le : Tableaux de bord interactifs.
â€¢	ConnectÃ© Ã  Prometheus (et Loki), permet de visualiser les mÃ©triques et logs.
â€¢	Accessible sur port 3000 (UI web).

ğŸ“œ Loki
â€¢	RÃ´le : SystÃ¨me de gestion de logs, Ã©quivalent "Prometheus pour les logs".
â€¢	Collecte les logs envoyÃ©s par Promtail et les rend consultables dans Grafana.
â€¢	ExposÃ© sur port 3100.

ğŸ•µï¸ Promtail
â€¢	RÃ´le : Agent de logs.
â€¢	Lit les fichiers de logs (/var/log) de lâ€™hÃ´te et/ou des conteneurs, et les envoie Ã  Loki.


docker-compose.yml : dÃ©finit et lance tout le systÃ¨me (frontend + orchestrateur + producteurs).

ğŸ“¦ DÃ©pendances Python

pyproject.toml : gÃ¨re les dÃ©pendances Python via [uv](https://github.com/astral-sh/uv).  


```text
â”œâ”€â”€ frontend
â”‚   â”œâ”€â”€ src/app
â”‚   â”‚   â”œâ”€â”€ app.component.html       # Frontend UI template
â”‚   â”‚   â”œâ”€â”€ app.component.ts         # Angular component logic
â”‚   â”‚   â””â”€â”€ event.service.ts         # SSE event handling
â”‚   â”œâ”€â”€ Dockerfile                   # Docker build for frontend
â”‚   â””â”€â”€ nginx.conf                   # Nginx config with SSE proxy
â”œâ”€â”€ grafana                          # Dossier de configuration pour Grafana (visualisation)
â”‚   â”œâ”€â”€ dashboards
â”‚   â”‚   â”œâ”€â”€ kafka_events_dashboard.json   # Tableau de bord pour les mÃ©triques Kafka (Prometheus)
â”‚   â”‚   â””â”€â”€ loki_events_dashboard.json    # Tableau de bord pour les logs (Loki)
â”‚   â””â”€â”€ provisioning
â”‚   â”‚   â”œâ”€â”€ dashboards
â”‚   â”‚   â”‚   â””â”€â”€ dashboard.yml        # Configuration des dashboards Grafana
â”‚   â”‚   â””â”€â”€ datasources
â”‚   â”‚       â””â”€â”€ datasource.yml       # Configuration des sources de donnÃ©es (Prometheus & Loki)
â”œâ”€â”€ orchestrator                     # Microservice Orchestrateur
â”‚   â”œâ”€â”€ app.py                       # FastAPI orchestrator
â”‚   â”œâ”€â”€ kafka_client.py              # Kafka producer/consumer logic
â”‚   â”œâ”€â”€ models.py                    # Pydantic event models
â”‚   â”œâ”€â”€ orchestrator.py              # Dependency graph and orchestration logic
â”‚   â””â”€â”€ utils.py                     # Utility for loading JSON
â”œâ”€â”€ producers                        # Microservices Producteurs
â”‚   â”œâ”€â”€ producer_ingest.py           # Ingestion simulator
â”‚   â”œâ”€â”€ producer_standardize.py      # Standardization simulator
â”‚   â””â”€â”€ producer_application.py      # Application simulator
â”œâ”€â”€ prometheus                       # Dossier de configuration pour Prometheus (collecte des mÃ©triques)
â”‚   â””â”€â”€ prometheus.yml               # Configuration de Prometheus (jobs de scraping)
â”œâ”€â”€ docker-compose.yml               # Orchestrates all services
â”œâ”€â”€ Dockerfile                       # Orchestrator Docker build
â”œâ”€â”€ Objets.json                      # Input data with object configurations
â”œâ”€â”€ promtail-config.yaml             # Configuration de Promtail (pour l'envoi des logs vers Loki)
â”œâ”€â”€ pyproject.toml                   # Python dependencies managed with uv
```

---
# ObservabilitÃ© : MÃ©triques et Logs
Ce projet est instrumentÃ© avec une pile d'observabilitÃ© complÃ¨te, permettant de surveiller les performances et de diagnostiquer les problÃ¨mes en temps rÃ©el.

Prometheus : Un systÃ¨me de surveillance et d'alerte. Il collecte les mÃ©triques (comme le nombre total d'Ã©vÃ©nements publiÃ©s) en interrogeant pÃ©riodiquement les services instrumentÃ©s.

Promtail : Un agent de log qui envoie les logs des conteneurs Ã  Loki. Il est configurÃ© pour lire les logs de chaque service.

Loki : Un systÃ¨me de stockage de logs. Il agrÃ¨ge et indexe les logs envoyÃ©s par Promtail, ce qui permet de les rechercher efficacement.

Grafana : Une plateforme de visualisation. Il se connecte Ã  Prometheus (pour les mÃ©triques) et Ã  Loki (pour les logs) pour afficher des tableaux de bord unifiÃ©s.

Flux ObservÃ© : RÃ´les et ResponsabilitÃ©s
Ce document dÃ©crit le flux de traitement des objets dans le systÃ¨me, les rÃ´les de chaque composant et les Ã©vÃ©nements Kafka Ã©changÃ©s.

1. Orchestrateur : Publication de ObjectReady pour 1001
Qui : L'orchestrateur
ImplÃ©mentation : orchestrator/orchestrator.py et orchestrator/app.py

Quoi :

Charge Objets.json et construit un graphe de dÃ©pendances avec NetworkX.

Identifie l'objet 1001 comme source (sans dÃ©pendance, IdObjet_Parent = null).

Effectue un tri topologique pour dÃ©terminer l'ordre de traitement.

Publie un Ã©vÃ©nement Kafka ObjectReady pour l'objet 1001 sur le topic object.events via KafkaClient (kafka_client.py).

Pourquoi :
1001 est le premier nÅ“ud dans l'ordre topologique, car il n'a pas de dÃ©pendances.

2. Producteur d'ingestion : Traitement de 1001
Qui : Producteur d'ingestion
ImplÃ©mentation : producers/producer_ingest.py

Quoi :

Consomme les Ã©vÃ©nements du topic object.events via AIOKafkaConsumer.

VÃ©rifie si l'Ã©vÃ©nement est ObjectReady et si id_objet (1001) est dans sa plage (1000-1999).

Simule le traitement de l'objet 1001 (asyncio.sleep(0.5)).

Publie un Ã©vÃ©nement ObjectLoaded pour 1001 sur le topic object.events via AIOKafkaProducer.

Pourquoi :
Ce producteur gÃ¨re les objets sources (1000-1999) et simule le travail d'ingestion (extraction de donnÃ©es brutes).

3. Orchestrateur : Publication de ObjectReady pour 2002
Qui : L'orchestrateur

Quoi :

Suit l'ordre topologique du graphe.

Identifie que 2002 est prÃªt Ã  Ãªtre traitÃ© (ses dÃ©pendances, comme 1001, sont rÃ©solues).

Publie ObjectReady pour 2002 sur le topic object.events.

Pourquoi :
L'ordre topologique garantit que les dÃ©pendances de 2002 sont traitÃ©es avant.

4. Producteur de standardisation : Traitement de 2002
Qui : Producteur de standardisation
ImplÃ©mentation : producers/producer_standardize.py

Quoi :

Consomme les Ã©vÃ©nements du topic object.events.

VÃ©rifie si l'Ã©vÃ©nement est ObjectReady et si id_objet (2002) est dans sa plage (2000-2999).

Simule le traitement de standardisation (asyncio.sleep(0.6)).

Publie ObjectLoaded pour 2002.

Pourquoi :
GÃ¨re la transformation des objets intermÃ©diaires (2000-2999).

5. Publication et traitement parallÃ¨le pour 3005 et 3006
Qui :

Orchestrateur : publication des ObjectReady

Producteur d'application : traitement des objets

Quoi :

Orchestrateur :

Publie ObjectReady pour 3005 et 3006 (dÃ©pendances rÃ©solues).

Producteur d'application (producers/producer_application.py) :

Consomme ObjectReady.

VÃ©rifie que les objets (3005, 3006) sont dans sa plage (3000+).

Simule le traitement (asyncio.sleep(0.8) par objet).

Publie ObjectLoaded.

Peut traiter plusieurs objets en parallÃ¨le grÃ¢ce Ã  la consommation asynchrone Kafka.

Pourquoi :
Ces objets sont terminaux (couche application), Kafka permet un traitement parallÃ¨le.

6. Gestion des Ã©checs (exemple 2005)
Qui : Producteur de standardisation

Quoi :

Simule un Ã©chec forcÃ© sur 2005.

Publie ObjectFailed sur le topic object.events avec mÃ©tadonnÃ©es d'erreur ("error": "test forcÃ© de objectfailed").

Pourquoi :
Tester la gestion des erreurs et la journalisation dans le systÃ¨me.

---

## 7. RÃ©sumÃ© des RÃ´les

| Composant | RÃ´le principal | Plage d'objets | Temps de traitement |
|-----------|----------------|----------------|------------------|
| Orchestrateur | Publie `ObjectReady` en ordre topologique | Tous | - |
| Producteur d'ingestion | Traite les objets sources | 1000-1999 | 0.5s |
| Producteur de standardisation | Traite objets intermÃ©diaires | 2000-2999 | 0.6s |
| Producteur d'application | Traite objets terminaux | 3000+ | 0.8s |
| Frontend | Affiche Ã©vÃ©nements Kafka en temps rÃ©el via SSE | - | - |
| Kafka | Communication entre producteurs et orchestrateur | - | - |

---

## 8. Endpoints API de l'orchestrateur

- `/orchestrate` : dÃ©clenche le processus d'orchestration  
- `/events/last` : rÃ©cupÃ¨re le dernier Ã©vÃ©nement  
- `/events/stream` : stream SSE des Ã©vÃ©nements en temps rÃ©el  

---

## 9. Architecture Kafka

- Tous les Ã©changes d'Ã©vÃ©nements se font via Kafka (`object.events`)  
- Permet un traitement asynchrone et parallÃ¨le des objets

---

## ğŸš€ Lancer le projet

### PrÃ©requis
Docker et Docker Compose
Python 3.11 (pour le dÃ©veloppement local, mÃªme si Docker gÃ¨re le runtime)
Node.js 20 (pour la compilation du frontend, pris en charge par Docker)

### Ã‰tapes

#### Cloner le dÃ©pÃ´t
```bash
git clone <url-du-repo>
cd kafka-orchestrator
```
#### Construire et lancer avec Docker Compose
```bash
docker compose up --build
```
#### Services dÃ©marrÃ©s
ğŸ¦“ Zookeeper : port 2181

ğŸ“¨ Kafka : port 9092

âš™ï¸ Orchestrateur : port 8000

ğŸ”„ Producteurs : ingest, standardize, application

ğŸ“Š Prometheus : port 9090 (collecte les mÃ©triques)

ğŸ“ Promtail : envoie les logs Ã  Loki

ğŸ” Loki : port 3100 (stocke les logs)

ğŸ“ˆ Grafana : port 3000 (visualise les mÃ©triques et les logs)

ğŸ’» Frontend : port 4200 (visualisation en temps rÃ©el des Ã©vÃ©nements)
